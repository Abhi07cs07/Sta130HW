{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f7fffd",
   "metadata": {},
   "source": [
    "Question 2: Real-World Applications of Accuracy, Sensitivity, Specificity, and Precision\n",
    "Accuracy\n",
    "Accuracy is the proportion of correctly classified instances (both true positives and true negatives) over the total number of instances. This metric is useful in situations where the cost of false positives and false negatives is equal. For example, in predicting whether a student passes or fails an exam based on their study hours and attendance, accuracy is sufficient because both false positives (predicting they will pass when they won’t) and false negatives (predicting they will fail when they won’t) hold equal weight. In this context, the goal is to assess the overall performance of the model across all types of errors.\n",
    "\n",
    "However, accuracy becomes less meaningful in imbalanced datasets. For instance, in fraud detection, where fraudulent transactions are rare, a model could achieve high accuracy by simply predicting all transactions as non-fraudulent. This highlights the need for alternative metrics in such cases.\n",
    "\n",
    "Sensitivity (Recall)\n",
    "Sensitivity measures the proportion of actual positive cases correctly identified by the model. It is especially useful in applications where missing positive cases is more costly than mistakenly identifying negatives as positives. For instance, in medical diagnostics, sensitivity is critical when detecting life-threatening diseases like cancer. Failing to diagnose a disease (a false negative) can have severe consequences, so the focus is on maximizing sensitivity even if it comes at the expense of false positives. In such scenarios, a model with high sensitivity ensures that almost all actual cases are detected.\n",
    "\n",
    "Specificity\n",
    "Specificity measures the proportion of actual negative cases correctly identified by the model. It is particularly relevant in scenarios where false positives are more harmful than false negatives. For example, in criminal justice, when predicting whether someone is guilty of a crime, specificity is crucial. False positives (wrongly predicting guilt) can lead to the wrongful imprisonment of innocent individuals. High specificity minimizes such errors by ensuring that actual negatives are correctly identified. Specificity is also important in spam email filters, where mistakenly labeling legitimate emails as spam can disrupt communication.\n",
    "\n",
    "Precision\n",
    "Precision measures the proportion of positive predictions that are actually correct. This metric is critical in situations where false positives have significant consequences. For example, in search engines, precision determines the relevance of the top search results. If a search query for “climate change” returns irrelevant links, users may lose trust in the search engine. Similarly, in targeted advertising, high precision ensures that ads are shown only to users who are genuinely interested, preventing wasted resources on uninterested audiences. High precision indicates that positive predictions are highly reliable.\n",
    "\n",
    "In summary, each metric is valuable in specific contexts. Accuracy is a general-purpose measure, while sensitivity and specificity address the balance between false positives and false negatives. Precision is key in applications where the reliability of positive predictions is critical.\n",
    "\n",
    "\n",
    "\n",
    "Question 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create 80/20 train-test split\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(ab_reduced_noNaN, test_size=0.2, random_state=42)\n",
    "\n",
    "# Report observation counts\n",
    "print(\"Training observations:\", ab_reduced_noNaN_train.shape[0])\n",
    "print(\"Testing observations:\", ab_reduced_noNaN_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052fc70",
   "metadata": {},
   "source": [
    "Description of the Process\n",
    "The task involves using a dataset of books to predict whether a book is a hardcover or paperback based on the variable \"List Price.\" First, the data is split into a training set (80%) and a testing set (20%). This ensures that the model is trained on a subset of the data and evaluated on unseen data to test its generalizability. The \"List Price\" variable is the only predictor used, and the decision tree classifier is limited to a depth of 2 to ensure simplicity and interpretability.\n",
    "\n",
    "Training the Decision Tree Classifier:\n",
    "The decision tree classifier is trained on the training data (ab_reduced_noNaN_train). Using DecisionTreeClassifier.fit(...), the model learns to split the data at decision nodes based on thresholds of \"List Price\" to classify books into hardcover or paperback categories. The max depth of 2 ensures that the tree has at most two decision nodes, reducing the risk of overfitting.\n",
    "\n",
    "Visualizing the Decision Tree:\n",
    "The tree.plot_tree(...) function provides a graphical representation of the fitted tree, showing the splits made at each decision node and the resulting classification outcomes. This visualization helps to interpret the decision-making process of the tree. For example, the tree might show a threshold split at \"List Price < $20,\" with books below this price classified as paperbacks and those above classified as hardcovers.\n",
    "\n",
    "Evaluating the Model:\n",
    "The performance of the model is assessed using the testing data (ab_reduced_noNaN_test). Metrics such as accuracy are calculated to determine how well the model generalizes to unseen data. This process ensures that the model is not overfitting to the training data and provides reliable predictions.\n",
    "\n",
    "Relevance of this Approach\n",
    "Using \"List Price\" as the sole predictor allows for a straightforward analysis, making the decision tree interpretable. The max depth constraint prevents the tree from becoming too complex and overfitting to the training data. This simplicity is particularly beneficial for applications where interpretability is more critical than maximizing accuracy. However, the limitation of using only one predictor highlights the need for more comprehensive models in future analyses.\n",
    "\n",
    "\n",
    "\n",
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c07bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, recall_score\n",
    "\n",
    "# Define the outcome variable and predictions\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "\n",
    "# clf confusion matrix\n",
    "clf_preds = clf.predict(ab_reduced_noNaN_test[['List Price']])\n",
    "cm_clf = confusion_matrix(y_test, clf_preds, labels=[0, 1])\n",
    "ConfusionMatrixDisplay(cm_clf, display_labels=[\"Paper\", \"Hard\"]).plot()\n",
    "\n",
    "# clf2 confusion matrix\n",
    "clf2_preds = clf2.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n",
    "cm_clf2 = confusion_matrix(y_test, clf2_preds, labels=[0, 1])\n",
    "ConfusionMatrixDisplay(cm_clf2, display_labels=[\"Paper\", \"Hard\"]).plot()\n",
    "\n",
    "# Metrics for each model\n",
    "print(\"clf - Accuracy:\", accuracy_score(y_test, clf_preds))\n",
    "print(\"clf - Sensitivity (Recall):\", recall_score(y_test, clf_preds))\n",
    "print(\"clf2 - Accuracy:\", accuracy_score(y_test, clf2_preds))\n",
    "print(\"clf2 - Sensitivity (Recall):\", recall_score(y_test, clf2_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d2757",
   "metadata": {},
   "source": [
    "Calculating Metrics for clf and clf2\n",
    "Two decision tree classifiers (clf and clf2) are evaluated on their ability to predict whether a book is a hardcover or paperback based on different sets of predictor variables. The metrics calculated include sensitivity, specificity, and accuracy.\n",
    "\n",
    "Sensitivity:\n",
    "Sensitivity measures the proportion of actual hardcovers correctly classified by the model.\n",
    "A higher sensitivity indicates that the model is effectively identifying hardcovers, minimizing false negatives.\n",
    "\n",
    "Specificity:\n",
    "Specificity measures the proportion of actual paperbacks correctly classified by the model.\n",
    " \n",
    "This metric highlights the model's ability to avoid false positives, which is crucial in scenarios where incorrect classifications of paperbacks as hardcovers are costly.\n",
    "\n",
    "Accuracy:\n",
    "Accuracy is the proportion of correctly classified books (both hardcovers and paperbacks) out of the total books. It provides an overall measure of model performance.\n",
    " \n",
    "Comparing the accuracy of clf and clf2 helps assess the impact of using additional predictor variables in clf2.\n",
    "\n",
    "Analysis of Results\n",
    "The results of the metrics reveal the trade-offs between sensitivity, specificity, and accuracy. clf2, which uses multiple predictors (e.g., \"NumPages,\" \"Thick,\" and \"List Price\"), is likely to outperform clf (using only \"List Price\") in accuracy and specificity due to the additional information provided by the predictors. However, the increased complexity of clf2 may lead to overfitting, as it might model idiosyncrasies in the training data that do not generalize well to the testing data.\n",
    "\n",
    "\n",
    "Question 7: Differences Between Two Confusion Matrices\n",
    "\n",
    "Explanation of Differences:\n",
    "The confusion matrices for clf and clf2 differ because clf uses a single predictor variable (\"List Price\"), while clf2 uses multiple predictors (\"NumPages,\" \"Thick,\" and \"List Price\"). These differences in predictors affect the model's ability to classify books accurately.\n",
    "\n",
    "clf Matrix (Single Predictor):\n",
    "\n",
    "The simplicity of clf means it is less likely to overfit the training data, but it also has limited predictive power due to the reliance on a single variable.\n",
    "This limitation is evident in the confusion matrix, where false positives and false negatives may be more frequent, resulting in lower sensitivity and specificity.\n",
    "clf2 Matrix (Multiple Predictors):\n",
    "\n",
    "By incorporating additional predictors, clf2 captures more complex relationships in the data, leading to improved classification accuracy.\n",
    "The confusion matrix for clf2 shows fewer false positives and false negatives compared to clf, reflecting the improved sensitivity and specificity.\n",
    "Conclusion\n",
    "While clf is simpler and less prone to overfitting, its predictive accuracy is limited. In contrast, clf2 benefits from the additional predictors, improving performance metrics. However, the complexity of clf2 introduces a risk of overfitting, which must be monitored by comparing training and testing performance metrics. These differences highlight the trade-offs between model complexity and interpretability, as well as the importance of evaluating models on unseen data to ensure generalizability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
